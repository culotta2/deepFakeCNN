{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, DirectoryIterator\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from pathlib import Path\n",
    "\n",
    "# Helper functions\n",
    "# from helper_functions import imgs_to_numpy\n",
    "\n",
    "# Styles for miles\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GimmeImages(data_split, batch_size = 100):\n",
    "    \n",
    "    DATADIR = Path('data/') / 'real_vs_fake' / 'real-vs-fake' / data_split\n",
    "    \n",
    "    if data_split == \"test\":\n",
    "        imgs = DirectoryIterator(DATADIR,\n",
    "                             image_data_generator = ImageDataGenerator(rescale = 1./255),\n",
    "                             color_mode = 'rgb', \n",
    "                             classes = [\"real\", \"fake\"],\n",
    "                             target_size = (256, 256),\n",
    "                             batch_size = batch_size,\n",
    "                             class_mode = 'binary',\n",
    "                             shuffle = False)\n",
    "    else:\n",
    "        imgs = DirectoryIterator(DATADIR,\n",
    "                                 image_data_generator = ImageDataGenerator(rescale = 1./255,\n",
    "                                                                           horizontal_flip = True,\n",
    "                                                                           rotation_range = 20),\n",
    "                                 color_mode = 'rgb', \n",
    "                                 classes = [\"real\", \"fake\"],\n",
    "                                 target_size = (256, 256),\n",
    "                                 batch_size = batch_size,\n",
    "                                 class_mode = 'binary',\n",
    "                                 shuffle = True)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_imgs = GimmeImages('train')\n",
    "val_imgs   = GimmeImages('valid')\n",
    "test_imgs  = GimmeImages('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepfake = keras.Sequential([\n",
    "  keras.Input(shape = (256, 256, 3)),\n",
    "  layers.Conv2D(filters = 16, kernel_size = 3, strides = (1, 1), padding = 'same', activation = 'relu'),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  layers.Conv2D(filters = 32, kernel_size = 3, strides = (1, 1), padding = 'same', activation = 'relu'),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  layers.Conv2D(filters = 64, kernel_size = 3, strides = (1, 1), padding = 'same', activation = 'relu'),\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(units = 256, activation = 'relu'),\n",
    "  layers.Dropout(rate = 0.1),\n",
    "  layers.Dense(units = 128, activation = 'relu'), \n",
    "  layers.Dense(units = 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_deepfake.compile(optimizer = 'adam', \n",
    "#                       metrics = [keras.metrics.BinaryAccuracy(), \n",
    "#                                  keras.metrics.Precision(), \n",
    "#                                  keras.metrics.Recall()],\n",
    "#                       loss = keras.losses.BinaryCrossentropy(from_logits = True,\n",
    "#                                                              name = 'binary_crossentropy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deepfake.compile(optimizer = 'adam', \n",
    "                       metrics = [keras.metrics.BinaryAccuracy()],\n",
    "                       loss = keras.losses.BinaryCrossentropy(from_logits = True,\n",
    "                                                              name = 'binary_crossentropy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               16777472  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 16,834,081\n",
      "Trainable params: 16,834,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_deepfake.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 200 steps\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1599s 2s/step - loss: 0.5627 - binary_accuracy: 0.6880 - val_loss: 0.4966 - val_binary_accuracy: 0.7301\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1584s 2s/step - loss: 0.4260 - binary_accuracy: 0.7939 - val_loss: 0.4131 - val_binary_accuracy: 0.8105\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 1620s 2s/step - loss: 0.3496 - binary_accuracy: 0.8400 - val_loss: 0.3341 - val_binary_accuracy: 0.8389\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 1579s 2s/step - loss: 0.3002 - binary_accuracy: 0.8661 - val_loss: 0.2940 - val_binary_accuracy: 0.8738\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 1587s 2s/step - loss: 0.2590 - binary_accuracy: 0.8875 - val_loss: 0.2586 - val_binary_accuracy: 0.8794\n"
     ]
    }
   ],
   "source": [
    "model_fit = model_deepfake.fit(train_imgs,\n",
    "                               epochs = 5,\n",
    "                               validation_data = val_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: deepFakeModel1/assets\n"
     ]
    }
   ],
   "source": [
    "model_deepfake.save('deepFakeModel1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('deepFakeModel1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 200 steps\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 2821s 3s/step - loss: 0.0814 - binary_accuracy: 0.9688 - val_loss: 0.1368 - val_binary_accuracy: 0.9529\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 1860s 2s/step - loss: 0.0787 - binary_accuracy: 0.9699 - val_loss: 0.1395 - val_binary_accuracy: 0.9518\n",
      "Epoch 3/5\n",
      " 233/1000 [=====>........................] - ETA: 21:35 - loss: 0.0710 - binary_accuracy: 0.9729"
     ]
    }
   ],
   "source": [
    "model.fit(train_imgs,\n",
    "          epochs = 5,\n",
    "          validation_data = val_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.09015239672735334, 0.0831708754338324],\n",
       " 'binary_accuracy': [0.96482, 0.96746],\n",
       " 'val_loss': [0.13588262742385268, 0.14079494023695588],\n",
       " 'val_binary_accuracy': [0.9515, 0.94275]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"modelHistEpoch22_26\", model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: deepFakeModel1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('deepFakeModel_save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEJCAYAAACUk1DVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4VNX9+PH3bJnsyyQkYQ0YBJIQJDjsa4AfIlRBtMWCFJFWqwgo9atEUPmqKF9XWkW2AiICxo1SF6QGihsSEmhkU4QAFWQJJEBCkklm5p7fH5MMGZKQACHr5/U8eZ7cO+feOZ8s9zPnnHvP0SmlFEIIIZo8fV1XQAghRP0gCUEIIQQgCUEIIUQJSQhCCCEASQhCCCFKSEIQQggBSEJocgYNGsQf//jHuq6GaELatm3L888/X9fVENUgCaGRuPfee9HpdO6voKAgevfuzeeff+5R7uOPP+a1116ro1pemR07dmAwGOjWrVtdV0Vc4siRIx5/bxV9DRo0CIC0tDQeffTRuq2wqBZJCI1I//79OXHiBCdOnGDbtm1069aN0aNHk5mZ6S5jsVgIDAy87nUpLi6+5nMsXryYBx98kCNHjpCenl4Dtbp2NRFXQ6OUwm63e+xr3bq1+2/txIkTvPnmmwAe+z7++GMAmjVrhp+fX63XW1w5SQiNiJeXF5GRkURGRhITE8O8efOw2+3s2rXLXebSLqPS7eeee47IyEgsFgv33nsv+fn57jI7d+7k1ltvJTw8HH9/f7p3784XX3zh8d5t27Zl9uzZPPTQQ4SGhtK3b18mTpzIsGHDytUzMTGRe++997Kx5OXlsXbtWu6//37uvvtulixZUq7MhQsXeOSRR2jdujVms5m2bdvywgsvuF/Pyspi0qRJRERE4O3tTceOHVm+fDkAW7ZsQafTcezYMY9zGo1G3n77beDip+DVq1czYsQI/Pz8ePLJJ1FK8ac//Yno6Gh8fHy44YYbePLJJykqKvI4V0pKCv3798fX15egoCAGDhxIZmYm//73vzEYDBw9etSj/MqVKwkICCAvL6/Sn8vKlSuJjY3FbDbTqlUrZs+ejcPhAGDp0qUEBQVRWFjoccz//d//0bJlSzRNA+DgwYPceeedBAcHExISwrBhw9i9e7e7/Ntvv43RaOTf//43CQkJmM1mNm7c6HFOg8Hg/luLjIwkKCgIwGOfxWIByncZtW3blqeeeooHH3yQoKAgwsPDefPNNykqKmLq1KmEhITQsmVLd5IpdeHCBaZPn07Lli3x9fUlISHBnXREDVGiUZg4caIaMmSIe7uoqEi9+uqrymw2qyNHjrj3Dxw4UE2ePNljOygoSD3yyCPqxx9/VBs2bFBBQUHq6aefdpf597//rd5++221d+9etX//fjVr1ixlMpnU/v373WWioqJUQECAeuaZZ9T+/fvV3r171datW5VOp1OHDh1ylzt48KDS6XTq22+/vWw8CxcuVAkJCUoppVJTU5W/v7/Ky8tzv65pmho4cKBq166dWrduncrMzFRfffWVWrJkiVJKqYKCAtWpUyeVkJCgvvzyS5WZmak2btyo1q5d644JUEePHvV4X4PBoFasWKGUUurw4cMKUC1btlSrVq1SmZmZ6tChQ8rpdKpZs2apbdu2qcOHD6v169eryMhIj5/Zl19+qfR6vZo+fbrKyMhQP/74o/r73/+ufvzxR6WUUh07dlRz5szxeO9+/fqpP/7xj5X+TD799FOl1+vVCy+8oPbv36/ee+89FRwcrGbPnq2UUurcuXPK29tbrVmzxuO4uLg49fjjjyullDp58qSKiIhQf/7zn9WuXbvUTz/9pB5++GFlsVhUVlaWUkqpFStWKJ1Op6xWq9q0aZPKzMx0v1aZVatWqcouJ1FRUeq5557z2A4KClKvvvqqOnDggHruueeUTqdTt956q3vfCy+8oHQ6ndq7d69SyvX7HjRokBo4cKD65ptvVGZmplq8eLEymUwqJSXlsnUT1ScJoZGYOHGiMhgMys/PT/n5+SmdTqf8/PxUcnKyR7mKEkJ8fLxHmQceeED16tXrsu/XpUsX9fzzz7u3o6Ki1ODBg8uVi4+PV7NmzXJvz5w5U8XGxlYZT0JCgpo/f757OzY2Vi1evNi9nZKSogCVlpZW4fF///vfldlsLnfBL3UlCeHZZ5+tsr6vvfaaat++vXu7X79+auTIkZWWf/XVV1WbNm2U0+lUSin1008/KUBt37690mP69eunfvvb33rsmz9/vvL29lZFRUVKKaXGjh2rhg8f7n59x44dClB79uxRSin1zDPPqJ49e3qcQ9M0dcMNN6jXX39dKeVKCID6+uuvq4y71JUmhFGjRrm3nU6nCggIUL/5zW889gUHB6s33nhDKeX6fZnNZnXu3DmPc0+aNMnjXOLaSJdRI9KzZ08yMjLIyMhg586dPP3000ycOLFcc/9SXbt29dhu2bIlp06dcm+fPn2ahx56iE6dOhEcHIy/vz979+7lv//9r8dxPXr0KHfuBx54gBUrVuB0OnE4HLz99tv86U9/umx9tm/fzu7duxk3bpx738SJEz26jXbs2EFISAhWq7XCc+zYsYPY2FhatWp12feqjoriWrp0KT179iQiIgJ/f3+SkpI8fh47duyosLus1L333ktWVpb7d7N06VJuuukmunfvXukxe/fuZcCAAR77Bg4ciM1mc48T/eEPf+DLL7/k5MmTAKxatYqbb76ZuLg4wDXAu2PHDvz9/d1fAQEBHDlyhAMHDnic+3J1uVY33XST+3u9Xk+zZs3o0qWLx77w8HCysrLc9S4uLqZly5YedX/33XfL1VtcPWNdV0DUHB8fH9q3b+/e7tq1K5s2bWLu3LnccsstlR7n5eXlsa3T6dz9zeC6eP3yyy+89NJLtGvXDh8fH+6+++5yA6wVDRxOmDCBJ554gs8++wxN0zh79ix/+MMfLhvHkiVLcDgcNG/e3L1PKYWmaezcudN915FOp7vseS73ul6vd5+3lNPp9Ii7srg++OADpkyZwrx58xg4cCCBgYF88MEHzJo1q9rvb7FYuOuuu1i6dClDhw7lnXfeYc6cOZeNp6Jzlta/dP8tt9xCs2bNWL16NdOnT2ft2rU8+eST7vKapjFkyJBy/fOAexwAXGME3t7eVdbnaplMJo9tnU5X4b7S34emaQQFBZGWllbuXJf+/YqrJwmhkTMajRQUFFzTOb7++mteeuklbr/9dgDy8/M5dOgQnTt3rvLYwMBA7r77bpYuXYqmadx5553uwcaK5Obm8t5777FgwYJyn4anTZvGkiVLWLRoETfffDM5OTmkp6dX2Eq4+eabWb58OceOHauwlRAeHg7A8ePHad26NQAZGRkeCaIyX3/9NQkJCcyYMcO978iRI+Xef+PGjUydOrXS8zzwwAMkJiayaNEi8vPzGT9+/GXfNy4ujq+++oopU6Z41KV0YBtcF/Jx48bxzjvvEBMTQ05ODr///e/d5a1WK2+//TYtW7bEx8enyljrC6vVyrlz57DZbNX6uxNXR7qMGpHi4mJOnjzJyZMnyczM5K233mLjxo3ccccd13Tejh07snr1anbv3k1GRga///3vcTqd1T7+gQceYMOGDWzcuJH777//smXfffdddDodkyZNonPnzh5f99xzD2vWrCE/P5/BgwfTv39/xo4dy/r16zl8+DDfffcdf//73wH4/e9/T1RUFLfffjspKSkcPnyYTZs2kZycDED79u2Jiopizpw5/PTTT3z77bc8+uijVbY6Sn8eu3fvZv369WRmZvLXv/613N0uTz31FBs2bOCRRx5h165d7N+/n7fffpv9+/e7y/Tr14+OHTvy2GOP8bvf/c7jE3pFkpKS+Oijj5g3bx4///wz77//PnPmzOEvf/mLx6fkiRMnsmvXLmbNmsWtt95Ks2bN3K89/PDDOJ1ORo8ezTfffMORI0f49ttvmTVrFlu3bq0y9royePBghg4dypgxY1i3bh2HDh1ix44dvPHGGyxdurSuq9doSEJoRL755huaN29O8+bNiY+PZ8GCBcybN4+kpKRrOu+KFSvQNI0ePXowevRohg8ffkX9y927dyc+Pp7o6GgGDhx42bJLlizhN7/5TYWfXu+44w5sNhtr165Fp9Px2WefMWLECP785z/TsWNH7rnnHs6cOQOAr68vX331FZ07d+buu+8mJiaGKVOmuG/JNBqNJCcnk5WVRUJCAlOmTGHu3LnurqTLeeCBB5gwYQKTJk0iISGB1NTUct09w4YN4/PPPyc1NZWePXvSo0cPVq5cWa5b5E9/+hPFxcVVJkqAESNGsHz5clauXEnnzp159NFHeeihh3jmmWc8ynXp0oWuXbuSkZFRrnsuIiKC77//nrCwMMaMGUPHjh0ZP348//3vfz266OobnU7HP//5T8aMGcOMGTPo1KkTI0eO5LPPPiM6Orquq9do6FR12shCXAOHw0FUVBQzZszgL3/5S11Xp155/PHH2bBhg8dzAELUFRlDENeNpmlkZWWxePFiLly4IHMolXH+/Hl2797N0qVLef311+u6OkIAkhDEdfTLL7/Qrl07mjdvzooVK6rsI29KRo0aRWpqKmPHjq3yrishaot0GQkhhABkUFkIIUQJSQhCCCGABjiGcPz48as6LiwszH1LYlMhMTcNEnPjd63xtmjRolrlpIUghBACkIQghBCihCQEIYQQgCQEIYQQJSQhCCGEACQhCCGEKNHgbjsVQoimJP1UOrt+3kWXoC5YIypeIbCmSEIQQoh6wOawcb74POds5zhX5PrKOJ3BW7vewqk5MRvMJI9Mvq5JQRKCEELUEKUUefY8ztnOcb74PGeLznpc4C+94JctY3PaLnvuYmcx35/4XhKCEELUJrtm53zRec4VnXNfsM8Xn3dfyMtul339fNF5nKry1QS9Dd4EewcTYg4hyCuIqIAobgq7iWDvYILNwQR5BRFsDnaX+SX3F6ZtmYZds2PSm+jdvPd1jVsSghCiUVJKUeAouHgRL/NVerEvvaCX3T5XdI58e36l59WhI8gcdPHibQ6mTUAbj+1g72CCvYI9toO8gvA2el9RDPFh8UT4RbDrvIwhCCEETs3p8en8fNF5nKecHD1ztMKLfdkLvl2zV3peL72X+4IdZA6ihV8LYi2x7u0Qc4jH66XfB3oFYtAbai1+a4SV4XHDa2XuJkkIQohaUegoLPdJ/HxRSR96JZ/ezxWdI7c497Ln9Tf5X/wkbg6mY0hHgs3B7gt62Yt52S8fow86na6Wom8YJCEIIapNUxp5xXmVXtAruuCXfn+5QVODzuBx8Q7zCaN9cPsKL+RB5iDaRbaDAgg0B2LSm2rxJ9C4SUIQogkqdhZX2Y9e0evni8+jKa3S8/oYfTwu3u2C2pW7mJd+H2IOcW/7m/yv6NN6WGgYZ1TTmf66tkhCEKKBUkqRb88n/3w+h7MPe97OeMnF/dILfoGjoNLzlg6alr2QRwVGVdr1UvZibzaYa/EnIGqaJAQh6phDc5BbnMtZ29mL96qX3NpY6aBpyf3sDuWo9Lxeei9CvEPctzO2CmhF57DOF29v9A6ucOA00CsQvU5mtWmKJCEIUQOUUticNs7azpa7oF/6gNKlr+fZ8y577gBTwMVbGc3BNPdr7nGveqvQVhiKDe7XS7+8Dd4yaCquiCQEIcrQlEZucW65rpeqHk46V3SOImdRpec16ozue9GDzcGE+4TTIbiD5/3qlzycFOIdQqBXIEb95f9Nm9pykuL6kYQgGqUiZ1G5fvTLPZxUdp9CVXpeX6Ovx6fw6KDoSgdNy17s/Ux+8mld1HuSEES9pZTigv1Cte9XL+1+yS7MptBRWOl59To9QV5B7oePQswhtAtsV27QtKKHk7wMXrX4ExCidklCENedQ3NUeb96ZZ/eLzcvjNlg9nj4qE1AGyKDIvFW3h6f1i99QCnAK0AGTYWogCQEUS1KKQodhZe9X730gn/p6xfsFy577kCvQI9P5q38W1V6v3rZT+s+Rp9y55L+dCGuniSEJsapOSscNK3s4aSyZYq14krPa9KbPC7WkX6RdLJ0qvR+dff3XkG1Oi+MEKJykhAaqIoW0zhX7HnveiGFZOVmlZsX5nKDpn4mP4+L940hN1Y6yVfZL1+jrwyaCtHAVSshZGRksGLFCjRNY8iQIYwePdrj9dOnT7Nw4UJyc3Px9/dn6tSphIaGAnDmzBkWLVpEdnY2AElJSYSHh7NgwQL27duHr68vAFOmTKFt27Y1GFr9V3YxjYou6Fe7mIZepyfYHIzFx0KAKYBQn1Cig6PdDyNVNnAa6BUog6ZCNGFVJgRN01i2bBmzZ88mNDSUpKQkrFYrrVq1cpdZtWoVAwYMYNCgQezZs4c1a9YwdepUAN58803GjBlDly5dsNlsHp8iJ0yYQK9eva5DWLWrNhbTCDYHV7mYRukF39/kj16nl/50IcQVqTIhHDx4kMjISCIiIgDo06cPaWlpHgnh2LFjTJw4EYC4uDhefvll936n00mXLl0A8Pa+ssUhakp1Fqm+ksU0Lr3g15fFNIQQ4lpUmRBycnLc3T8AoaGhHDhwwKNMVFQUqampjBgxgu3bt1NYWEheXh7Hjx/Hz8+PV155haysLOLj4xk/fjx6veuWv7Vr1/Lhhx/SuXNnxo8fj8lUfhrblJQUUlJSAJg3bx5hYWFXFOC2Y9v47We/xe60Y9AbGN1xNN4Gb3JsOZwtPEuOLYdztnPkFOZcfjENgxcWbwshPiGEeIfQNqQtFh8LId6u7RCfEI/XQ7xDsPhYCDLX3aCp0Wi84p9XQycxNw1NLebairfKhKBU+QHISwcPJ0yYwPLly9myZQsxMTFYLBYMBgOapvHjjz/y0ksvERYWxuuvv86WLVsYPHgw48aNIzg4GIfDweLFi1m/fj133XVXufcaOnQoQ4cOdW9faRfIhp82UOx03R3j0Bz8c/8/CfcNd38abx/YnuBmFU/yVfbrihfT0EDL1zibf/aK6luTmmKXkcTcNDS1mK813hYtWlSrXJUJITQ01D0gDJCdnU1ISIhHGYvFwmOPPQaAzWYjNTUVX19fLBYL7dq1c3c39ejRg59//pnBgwe7z2EymUhMTOSTTz6pXmRXqHfz3pgNZhyaA5PeRPLI5Ou+LqkQQjREVT6uGR0dzYkTJ8jKysLhcLB161asVs8Lam5uLprmWjRj3bp1JCYmAtC+fXvy8/PJzXUtgbdnzx732MPZs65Pzkop0tLSaN26dc1FVYY1wsr7I99nzoA5kgyEEOIyqmwhGAwG7rvvPubOnYumaSQmJtK6dWuSk5OJjo7GarWyb98+1qxZg06nIyYmhsmTJwOg1+uZMGECzz77LEopbrjhBnf3z9/+9jd3ooiKiuL++++/bkHW5iLVQgjRUOlURYME9djx48ev6rim1ucIEnNTITE3frU1hiAzfAkhhAAkIQghhCghCUEIIQQgCUEIIUQJSQhCCCEASQhCCCFKSEIQQggBSEIQQghRQhKCEEIIQBKCEEKIEpIQhBBCAJIQhBBClJCEIIQQApCEIIQQooQkBCGEEIAkBCGEECUkIQghhAAkIQghhCghCUEIIQQgCUEIIUQJSQhCCCEASQhCCCFKSEIQQggBSEIQQghRQhKCEEIIQBKCEEKIEpIQhBBCAJIQhBBClJCEIIQQApCEIIQQooQkBCGEEAAYq1MoIyODFStWoGkaQ4YMYfTo0R6vnz59moULF5Kbm4u/vz9Tp04lNDQUgDNnzrBo0SKys7MBSEpKIjw8nKysLObPn8+FCxdo164dU6dOxWisVnWEEEJcB1VegTVNY9myZcyePZvQ0FCSkpKwWq20atXKXWbVqlUMGDCAQYMGsWfPHtasWcPUqVMBePPNNxkzZgxdunTBZrOh0+kAePfddxk5ciR9+/ZlyZIlbN68mWHDhl2nMIUQQlSlyi6jgwcPEhkZSUREBEajkT59+pCWluZR5tixY8THxwMQFxdHenq6e7/T6aRLly4AeHt7YzabUUqxd+9eevXqBcCgQYPKnVMIIUTtqrKFkJOT4+7+AQgNDeXAgQMeZaKiokhNTWXEiBFs376dwsJC8vLyOH78OH5+frzyyitkZWURHx/P+PHjuXDhAr6+vhgMBgAsFgs5OTkVvn9KSgopKSkAzJs3j7CwsKsL1Gi86mMbKom5aZCYG7/airfKhKCUKrevtNun1IQJE1i+fDlbtmwhJiYGi8WCwWBA0zR+/PFHXnrpJcLCwnj99dfZsmULVqu12hUcOnQoQ4cOdW+fOXOm2seWFRYWdtXHNlQSc9MgMTd+1xpvixYtqlWuyoQQGhrqHhAGyM7OJiQkxKOMxWLhscceA8Bms5Gamoqvry8Wi4V27doREREBQI8ePfj5559JTEykoKAAp9OJwWAgJycHi8VS7eCEEELUvCrHEKKjozlx4gRZWVk4HA62bt1a7hN+bm4umqYBsG7dOhITEwFo3749+fn55ObmArBnzx5atWqFTqcjLi6Obdu2AVxxq0EIIUTNq7KFYDAYuO+++5g7dy6appGYmEjr1q1JTk4mOjoaq9XKvn37WLNmDTqdjpiYGCZPngyAXq9nwoQJPPvssyiluOGGG9zdP+PHj2f+/Pm89957tGvXjsGDB1/fSIUQQlyWTlU0SFCPHT9+/KqOa2p9jiAxNxUSc+NXW2MI8qSyEEIIQBKCEEKIEpIQhBBCAJIQhBBClJCEIIQQApCEIIQQooQkBCGEEIAkBCGEECUkIQghhAAkIQghhCghCUEIIQQgCUEIIUQJSQhCCCEASQhCCCFKSEIQQggBSEIQQghRQhKCEEIIQBKCEEKIEpIQhBBCAJIQhBBClJCEIIQQApCEIIQQooQkBCGEEIAkBCGEECUkIQghhAAkIQghhCghCUEIIQQgCUEIIUQJSQhCCCEASQhCCCFKSEIQQggBgLE6hTIyMlixYgWapjFkyBBGjx7t8frp06dZuHAhubm5+Pv7M3XqVEJDQwEYO3Ysbdq0ASAsLIwnnngCgAULFrBv3z58fX0BmDJlCm3btq2puIQQQlyhKhOCpmksW7aM2bNnExoaSlJSElarlVatWrnLrFq1igEDBjBo0CD27NnDmjVrmDp1KgBeXl68/PLLFZ57woQJ9OrVq4ZCEUIIcS2q7DI6ePAgkZGRREREYDQa6dOnD2lpaR5ljh07Rnx8PABxcXGkp6dfn9oKIYS4bqpsIeTk5Li7fwBCQ0M5cOCAR5moqChSU1MZMWIE27dvp7CwkLy8PAICArDb7cycORODwcCoUaPo0aOH+7i1a9fy4Ycf0rlzZ8aPH4/JZCr3/ikpKaSkpAAwb948wsLCri5Qo/Gqj22oJOamQWJu/Gor3ioTglKq3D6dTuexPWHCBJYvX86WLVuIiYnBYrFgMBgAeOutt7BYLJw6dYpnn32WNm3aEBkZybhx4wgODsbhcLB48WLWr1/PXXfdVe69hg4dytChQ93bZ86cueIgwTV+cbXHNlQSc9MgMTd+1xpvixYtqlWuyoQQGhpKdna2ezs7O5uQkBCPMhaLhcceewwAm81Gamqqe7DYYrEAEBERQWxsLEeOHCEyMtJ9DpPJRGJiIp988km1KiyEEOL6qHIMITo6mhMnTpCVlYXD4WDr1q1YrVaPMrm5uWiaBsC6detITEwE4MKFC9jtdneZ/fv3uwejz549C7haIGlpabRu3brmohJCCHHFqmwhGAwG7rvvPubOnYumaSQmJtK6dWuSk5OJjo7GarWyb98+1qxZg06nIyYmhsmTJwPw66+/smTJEvR6PZqmMXr0aHdC+Nvf/kZubi7gGoO4//77r2OYQgghqqJTFQ0S1GPHjx+/quOaWp8jSMxNhcTc+NXWGII8qSyEEAKQhCCEEKKEJAQhhBCAJAQhhBAlJCEIIYQAJCEIIYQoIQlBCCEEIAlBCCFECUkIQgghAEkIQgghSkhCEEIIAUhCEEIIUUISghBCCEASghBCiBKSEIQQQgCSEIQQQpSQhCCEEAKQhCCEEKKEJAQhhBCAJAQhhBAljHVdgdpgSk9Hv2sXpi5dsFutdV0dIYSolxp9QjClpxM2Zgw4nYQZjeQmJVFw992o4OC6rpoQQtQrjT4hmL//HjQNHaAcDoKee46g557D3rEjxVar66t7d5xt24JOV9fVFUKIOtPoE0JR7974m81gt6NMJs7Pno0+Nxev9HR8PvkEv9WrAXA2a+aRIOzx8eDlVce1F0KI2tPoE4LdaiU7OZmQXbs4e+kYgqZh/PlnvNLSXF/p6fhs2ACA8vam+KabKO7eneKbb6bYakVZLHUUhRBCXH+NPiGAKylow4djP3PG8wW9HkenTjg6daJgwgTXrqwsd3LwSkvDf/FidHa76zzt27sSRPfuFFutOG+4QbqZhBCNRpNICFdCCw/HNnIktpEjXTsKC/H64Qd3K8Jnwwb81q4FwBkaSrHVir2km6m4Sxcwm+uw9kIIcfUkIVTFx4fiXr0o7tXLta1pGA8edLcgvNLS8Nm4EQDl5YW9SxePVoQWGlqHlRdCiOqThHCl9HocHTrg6NCBgnHjXLvOnPFIEH7LluG/cCEAjhtucA9UF3fvjqN9e+lmEkLUS5IQaoAWFoZt+HBsw4e7dthseO3ahVd6Oqa0NMxffonv+++7ygYHeySI4i5dwMenDmsvhBAukhCuB29vinv0oLhHD9e2UhgyMy+2ItLTCUxJcb1kMmHv3PligujeHa1ZszqsvBCiqapWQsjIyGDFihVomsaQIUMYPXq0x+unT59m4cKF5Obm4u/vz9SpUwkt6TsfO3Ysbdq0ASAsLIwnnngCgKysLObPn8+FCxdo164dU6dOxWhspPlJp8PZvj2F7dtTePfdAOhzcjClp7uThN/KlfgvWQKAo21bz26mG28EvUw7JYS4vqq8AmuaxrJly5g9ezahoaEkJSVhtVpp1aqVu8yqVasYMGAAgwYNYs+ePaxZs4apU6cC4OXlxcsvv1zuvO+++y4jR46kb9++LFmyhM2bNzNs2LAaDK1+0ywWioYNo6g05qIiTLt3uxNPwJ+tAAAfMUlEQVSE+d//xvfDD11lg4Lcz0IUd++OPSEBJd1MQogaVmVCOHjwIJGRkURERADQp08f0tLSPBLCsWPHmDhxIgBxcXEVJoCylFLs3buX6dOnAzBo0CA++OCDJpUQyjGbsZfcwpr/5z+7upmOHPF4JiJw82YAlNHo6mYqbUVYrWiRkXUcgBCioasyIeTk5Li7fwBCQ0M5cOCAR5moqChSU1MZMWIE27dvp7CwkLy8PAICArDb7cycORODwcCoUaPo0aMHeXl5+Pr6YjAYALBYLOTk5FT4/ikpKaSU9LfPmzePsLCwqwvUaLzqY+tMs2bQvTsACijOyUG3bRv677/HuG0bptWr8f/7312vR0Wh9emD6t0b1acPKja2YcZ8jSTmpqGpxVxb8VaZEJRS5fbpLrltcsKECSxfvpwtW7YQExODxWJxX+zfeustLBYLp06d4tlnn6VNmzb4+vpWu4JDhw5l6NCh7u0zlz5tXE1hYWFXfWy90qOH62v6dCguxrR378WpNzZtwljy0JwWEAC9emErmX7DnpCA8vOr48pff43m93wFJObG71rjbdGiRbXKVZkQQkNDyc7Odm9nZ2cTEhLiUcZisfDYY48BYLPZSE1NdV/0LSXz/0RERBAbG8uRI0fo2bMnBQUFOJ1ODAYDOTk57nLiCnh5YU9IwJ6QQP7997u6mX755eIDcxkZBKSkoFMKZTBgj411dzEVd++OVs0/EiFE01BlQoiOjubEiRNkZWVhsVjYunUr06ZN8yhTeneRXq9n3bp1JCYmAnDhwgXMZjMmk4nc3Fz279/PqFGj0Ol0xMXFsW3bNvr27cuWLVuwysI1106nwxkVRWFUFIV33YUpLIzszEy8du50JwnftWvxX74cAEfLlh5PVTtiYqCkZSeEaHqqTAgGg4H77ruPuXPnomkaiYmJtG7dmuTkZKKjo7Farezbt481a9ag0+mIiYlh8uTJAPz6668sWbIEvV6PpmmMHj3aPRg9fvx45s+fz3vvvUe7du0YPHjw9Y20iVJBQRQlJlJUkqSx2zHt2+cerDZv24bvP/4BgObnh71bt4tJols3lL9/HdZeCFGbdKqiQYJ67Pjx41d1XFPrc4RqxqwUhl9/vTgOkZaG8aef0GkaSq/HERPj8UyEs2XLej31hvyem4amFnO9GUMQjZxOh7NVKwpbtaLwjjtcu/Ly8PrPfy6ORXz4IX4rVwLgjIz0eKraHhsLjfWBQiGaGPlPFuWogACKBgygaMAA1w6HA+NPP3m0Inw++QQAzccHe0LCxSRx882owMA6rL0Q4mpJQhBVMxpxdO6Mo3NnCiZNAkD/66+uB+ZKFxJ64w1XN5NOh6NTJ89uptat63U3kxDCRRKCuCpay5bYWrbENmoUALr8fEw7d7oThM+6dfitWgWAMyLCNfVGaTdT585gMtVl9YUQFZCEIGqE8vOjuH9/ivv3d+1wOjHu3+8x9YbP558DoHl7u7qZSlsRN9+MCg6uw9oLIUASgrheDAYcsbE4YmMpKJnnSn/y5MVxiPR0/N96C53TCYC9QwePh+acbdtKN5MQtUwSgqg1WmQktttuw3bbbQDoCgowZWS4E4TPJ5/gt3o1AM5mzVzJoXSG1/h48PKqy+oL0ehJQhB1Rvn6UtynD8V9+rh2aBrGn3/2aEX4bNjgKms2U1wyL1NpolAy3YkQNUoSgqg/9HocnTrh6NSJggkTXLuysjzWq/ZfsgTdggUA2Nu397jd1RkdXZe1F6LBk4Qg6jUtPBzbiBHYRoxw7SgsxOuHHzxaEH4lM7w6LRZ0ffrgX9qSiI8Hb+86rL0QDYskBNGw+PhQ3KsXxb16ubY1DWNm5sUH5nbuJPDTTwFQXl7Yu3TxnOG1zNoeQghPkhBEw6bX47jxRhw33kjBuHEYw8LI+eknj4fm/JYtw3/hQgAc7dp5TL3hiI6W9aqFKCEJQTQ6WlgYtuHDsQ0f7tphs+G1ezdeaWmY0tIwp6Tg+/77rrLBwR53MxXfdBPIetWiiZKEIBo/b293iwBwzfCamYnXjh3urqbAkmValcnkWq+6TCtCa9asDisvRO2RhCCaHp0OZ/v2FLZvT+HYsQDoc3Iwle1mWrkS/yVLAHC0bevRinB06CDdTKJRkoQgBKBZLBQNG0bRsGGuHUVFmHbvdo9FmLdswffDD11lg4JcczOVPjSXkICSbibRCEhCEKIiZjN2qxW71Uo+uLqZjhzxmJspcPNmAJTR6OpmKjOBnxYZWafVF+JqSEIQojp0Opzt2lHYrh2Fv/uda9fZsxfXq05Px3f1avyXLQPA0bq1x+2ujo4dZb1qUe9JQhDiKqmQEIqGDKFoyBDXjuJiTHv3ugeqzd9+i+/HHwOgBQRQXLpetdWKvVs3lJ9fHdZeiPIkIQhRU7y8sCckYE9IIP/++13dTEePeszNFPDqq+iUQhkM2GNjPeZm0lq2rOsIRBMnCUGI60Wnw9mmDYVt2lB4552uXefPe3YzrV2L//LlADhatPB8aK5TJ1mvWtQq+WsTohapoCCKEhMpSkx07XA4MO3bd7GbKTUV3/XrAdD8/LCXdjN1705xQgIqIKAOay8aO0kIQtQloxF7ly7Yu3Qhf/JkVzfTr796LiQ0f75rveqS2WDLtiKc0s0kapAkBCHqE50OZ6tWFLZqReEdd7h25eXh9Z//XJzA78MP8Vu5EgBnZCT064dfly6uweq4OOlmEldNp5RSdV2JK3H8+HGPbaUUNpsNTdPQXWbJRbPZTFFR0fWuXr3SlGNWSqHX6/H29r7s30WD5HBg/Omni1OA79yJ7pdfANB8fFzrVZdZJ0IFBtZxhWteWFgYZ86cqetq1JprjbdFixbVKtfgE0JhYSEmkwljFZ+KjEYjDofjelat3mnqMTscDux2Oz6N/CnisLAwcnbt8nhozrRvHzqnE6XTubqZSifvs1pxtmnT4NerloRwZaqbEBp821LTtCqTgWiajEZjk2khaS1aYBs1CtuoUQDo8vMx7dzpnnrD5x//wG/VKgCc4eEXE0T37tg7dwaTqS6rL+qJBn8lbXTdAaJGNdW/D+XnR3H//hT37+/a4XRi3L/foxXh8/nnAGje3q5uptKpN6xWVHBwHdZe1JUG32VUUFCAr69vlcc19e6TpuLSmKv799GQXW13gv7kyYsJIj0d05496Ep+dvYOHTym3nC2bVuvupmky+jKNJkuo/qgdevWdOrUCaUUBoOB559/nu7du3Py5Emeeuopli5dWqv1cTgcdO3alXvuuYeZM2fW6nuLhkOLjMR2223YbrsNAF1BAaaMjIuD1Z9+it/q1QA4w8I8xiHs8fFgNtdl9cV10CRbCOmn0vn+xPf0bt4ba4T1mut04403cuDAAQC2bNnCG2+8wUcffXTN5y3L6XRiqObkaJs2beJvf/sbp0+f5rvvvrtu3SYOh6Pejd9IC6EGaRrGAwc8nokwHjkCgDKbKb7pJo+pN5TFUvN1qIS0EK5MjbYQMjIyWLFiBZqmMWTIEEaPHu3x+unTp1m4cCG5ubn4+/szdepUQsssZl5QUMCjjz5Kjx49mDx5MgBz5szh7NmzeHl5ATB79myCgoKqVenKPP390+zL3lfhazqdDqUUecV57Mveh4aGHj2xobEEeFX+9GdsaCzP9n622nXIy8tzx3H06FEmTpzI5s2bSU5O5ssvv6SwsJAjR45w6623Mnv2bABmzpzJDz/8gM1mY+TIkTz22GMA9OzZk7vvvpuvvvqKxMREPv/8czZu3AjAoUOHeOihh/jiiy/K1eEf//gHkydPZtWqVezYsQOr1ZX0MjIyePrppykoKMBsNpOcnIyPjw9z587lq6++QqfTMW7cOO677z569uzJhg0bsFgs/PDDDzz33HN8+OGHvPrqq5w6dYqjR49isViYOXMm06ZNo6CgAMDdOgJ46623+Oijj9DpdAwePJhx48bxwAMPVCsGUQ/o9Tg6dsTRsSMF99zj2pWV5R6D8EpLw3/JEnQLFgBgj46+eLur1YozOrpedTOJqlWZEDRNY9myZcyePZvQ0FCSkpKwWq20atXKXWbVqlUMGDCAQYMGsWfPHtasWcPUqVPdrycnJxMbG1vu3NOmTSM6OrqGQqme3OJcNDQANDRyi3MvmxCqw2az8f/+3/+jqKiIrKws3i9Zr/dSe/fuZePGjXh5eTFgwAAmTZpEy5YteeKJJwgJCcHpdDJ27Fj27dvn/nmZzWb+8Y9/APDNN9+wZ88eOnfuTHJyMr8rmYa5rMLCQr777jteeuklLly4wPr167FarRQXF/Pggw+ycOFCunbtSl5eHt7e3rz77rscPXqUjRs3YjQaOXv2bJXx7tq1i3Xr1uHj40NhYSFr167F29ubQ4cOMWXKFDZs2MDmzZv54osv+PTTT/Hx8eHs2bOEhIQQEBBQZQyi/tLCw7GNGIFtxAjXjsJCvEpveU1Lw+eLL/B77z0AnBaLq3upNEnEx4O3dx3WXlSlyoRw8OBBIiMjiYiIAKBPnz6kpaV5JIRjx44xceJEAOLi4nj55Zfdrx06dIjz58/TtWtXMjMza7r+Hi73Sb60KyH9VDpjPxuLXbNj0pt4I/GNa+428vb25ssvvwQgPT2d6dOns7lk8ZSy+vXrR2DJQ0IdOnTg119/pWXLlnzyySesXr0ap9PJqVOnOHDggDsh3H777e7jx40bx/vvv09MTAyffPIJn376abn3SElJoU+fPvj4+PCb3/yG1157jTlz5pCZmUl4eDhdu3YFIKBkTpxvv/2WCRMmuLt+QkJCqox32LBh7nv77XY7s2bNYt++fej1eg4dOgS4ktfYsWPd5UrPW50YRAPi40Nxz54U9+zp2tY0jJmZF7uZ0tLw+de/AFBeXtjj4z3Xqy7TkyDqXpUJIScnx6P7JzQ01N1fXioqKorU1FRGjBjB9u3bKSwsJC8vDz8/P9555x0efvhh9uzZU+7cb731Fnq9np49e3LnnXdW2NedkpJCSskC6PPmzSMsLMzj9VOnTlW7H9toNNKrZS8+vP1Dth7fSp8Wfege2b1ax1bn3AC9evUiJyeHc+fOufv8jUYjBoMBb29vdzmDwYBSil9//ZXFixezceNGgoODmTZtGna7HaPRiE6nIyAgwH3M7bffzuuvv07//v256aabCA8PL1ePf/7zn2zfvp1evXoBcO7cObZt20ZYWBh6vb7Cn5XBYCi3v/T9SxNp6fd6vR4/Pz93+WXLlhEREcGCBQvQNI02bdq4j63ovNWJ4VqVfU+z2Vzub6axMRqN9SvG8HDo3RsADSg+fRrd99+j//57jN9/j2n5cvwXLQJAtW+P1qcPqndvtN69oWPHaq1XXe9ivs5qK94qr6QVjTlfeuGeMGECy5cvZ8uWLcTExGCxWDAYDPzrX/8iISGhwkCmTZuGxWKhsLCQV199la+//pqBAweWKzd06FCGDh3q3r50YKWoqKhag61lBxsTwhJICEsAqLHbMkvPc/DgQZxOJ4GBgVy4cMH9mtPpRNM0dzmlFE6nk3PnzuHj44Ovry8nTpxg06ZN9OzZE4fD4S5TeozRaGTgwIE88cQTvPLKK+XqnpeXR2pqKmlpaZjNZoxGI6tXr+bjjz/mxRdf5OTJk6Snp9O1a1cuXLiAt7c3/fv35+2336Znz57uLqOQkBBatWrFf/7zHwYPHsw///lPlFI4HA40TfOI49y5czRv3hxN00hOTnbXt3///rz++uuMGjXKo8uoqhiu1aWDykVFRY1+8LHeD7DqdNCnj+sLwGbDa/du1xPVaWl4ffopxnfeAUALDvZYirT4ppuggifN633MNazeDCqHhoaSnZ3t3s7Ozi7XrWCxWNwDoTabjdTUVHx9ffn555/58ccf+de//oXNZsPhcODt7c348eOxlNyR4OPjQ79+/Th48GCFCaEhKB1DANeFfv78+dW+IyguLo7OnTuTmJhImzZt3AOylbnjjjvYsGFDhT+rzz//nL59+2IuczvgsGHDeP7553nhhRdYuHAhs2fPxmaz4e3tTXJyMuPGjePQoUMMHToUo9HI+PHjmTRpEjNmzOAvf/kLb7zxBgkJCZXWZ+LEidx///18+umn9O3b131HT2JiInv37uXWW2/FZDIxePBgkpKSqoxBNAHe3u4LPuCa4fXQIY/B6sBNm1wvmUyu9arLdjM1a1aHlW/cqrzt1Ol0Mn36dJ5++mksFgtJSUlMmzaN1q1bu8uU3l2k1+tZu3Yter2esWPHepxny5YtZGZmMnnyZJxOJ/n5+QQGBuJwOPjrX/9KfHw8w4YNq7LCTf3BtEWLFpGbm8vjjz9eZdn6GvOVxHCl5LbTxkGXk+N+YM4rLQ2vH35AVzINiSMqCl2/fuSVjEc4OnSoVjdTQ1ZvWggGg4H77ruPuXPnomkaiYmJtG7dmuTkZKKjo7Farezbt481a9ag0+mIiYlx31paGbvdzty5c93dKPHx8R7dQqJikydP5r///W+ldzE1BI0hBnH9KYuFomHDKCr9kFhcjKmkm8krPR3vL78kuOShOS0oyNXNVNLVZE9IQDXyDwHXS5N8MK2pkJilhdBYhYWGcra09VA69cb+/QAooxF7XJzHBH5aZGQd1/ja1JsWghBC1Ds6Hc527Shs147CkmdZdOfO4bVjx8X1qlevxn/ZMgAcrVu714dwr1ddzXG+pkQSghCiUVDBwRQNGULRkCGuHXY7pj173OMQ5u++w/fjjwHQAgIoLl2v2mrF3q0bys+vDmtfP0hCEEI0TiYT9oQE7AkJ5P/pT667mY4e9ZibKeDVV9EphTIYsMfGekzgpzXB9aolIQghmgadDmebNhS2aUPhnXe6dp0/77FetW9yMv4rVgDgaNHC43ZXR6dOjX696sYdXS246667ePjhhxk0aJB739KlSzl06BAvvvhipceVnSG1rPo6lfb48ePdzxEI0ViooCCKBg2iqPT/1+HAtG+fO0GYU1PxXb8eAM3PD3u3bhdbEd26oQKubR60+qZJJgRTejrm77+nqHdv7NZrm8do1KhRrF+/3iMhrF+/nqeeeuqqzld2XqQtW7Ywb948PvroIyIjI2ssGVzJVNpfffUV0dHRfPLJJ8ycObNJTaUtmiCjEXuXLti7dCF/8mRXN9Ovv3rO8PrXv6LTNJRe71qvukwrwtmyZYOe4bVR/QcGPv00pn2Xn/5al5fnKqNpBOj12GNjL5vl7bGx5D5b+aR5I0eO5KWXXqKoqAiz2czRo0c5deoUPXr0ID8/n0mTJnH+/HkcDgePP/44t9xyS7XjudaptG+77TZmzJgBXPtU2u+8806DmEq7dN4rIWqEToezVSsKW7WisGTaf11enkc3k8+HH+K3ciUAzshIz/Wq4+IaVDdTw6lpDdHn5oKmoQOUpqHPzcV5Dc0+i8VC165d2bJlC7fccgvr16/n9ttvR6fTYTabWbZsGQEBAeTk5HDbbbcxbNiwy37KrsmptO++++4am0o7NzdXptIWAlABARQNGEDRgAGuHQ4Hxp9+8mhF+JTM4qv5+LjWqy5tRXTrhrrGdV+up0aVEC73Sb70gSVTejqhY8eC3Y4ymTj7xhvX3G00evRo1q9f704Ir732GuCa12jevHmkpqai0+k4efIkp0+fvuwMnzU5lXZWVlaNTaU9YsQI5s+fL1NpC3EpoxFH5844Onem4N57AdAfP37xobm0NPzffBOd04nS6XB07OjRinC2aVNvupkaVUKoDrvVSnZyco2NIQAMHz6c//3f/2X37t3YbDbi4+MB+Pjjj8nOzmbDhg2YTCZ69uxJUcl8LNVhtVrJycnxmFywVOlKcwB6vR6Hw8Evv/zC4sWL+eyzzwgODmbGjBnYbDZ3ubJP7I4YMYLXXnuNvn37Eh8f755ssKz169eTlpZGz5K57s+ePct3331HWFhYha2cyh56NxqNaJprUaJL4y9bp6VLl9KsWTO+/PJLNE3jhhtucJ+3overTgxC1AWtRQtso0ZhGzUKAF1+PqbSbqb0dHzWr8fv3XcBcIaHe65X3bkzlPn/rk1NLiGAKynURCIo5efnR+/evZkxY4bH8qJ5eXmEhYVhMpn47rvvOHbs2BWdt3Qq7ZCQEAoLC6ssn5eXh4+PD4GBgZw+fdo9lXZFvL29GTRoEElJSbzyyisVnmv79u3uqbTBtfLd+vXrefHFFzl16hQZGRkeU2kPGDCAVatW0adPn3JTae/atYvBgwfz2WefVVr/3Nxcmjdvjl6v54MPPsDpdAIwcOBAXn/9de644w6PLqOqYhCivlB+fhT360dxv36uHU4nxv37Pabe8Pn8cwA0b2/sXbu6k4QyGNAfPoypS5cavW5VpEkmhOth9OjR/PGPf2ThwoXufWPGjGHixInceuutxMXF0b59+yrPU5NTaffo0eOy5WUqbSHqiMGAIzYWR2wsBSWrTepPnrw4DpGejv+iRejefBMABYSZzZx5//3rmhRkcrtGrKqYr+c01LXl0hhkcrumoSnErCssJPDpp/Fdu9b9NHXe//wPF8qsV19dMrmduKzGMA11Y4hBiMooHx8Kxo7F5+OP3TfBFJUsTXq9SAuhEZOYpYXQWDWlmE3p6YTs2sXZaxhDaDIthAaWz0Qtk78P0dDZrVa04cOx10ICbPDrzpXecinEpRwOB/pGvrSiEDWpwbcQvL29sdlsFBUVXfYJYLPZfEXPADQGTTlmpRR6vR5vb++6rpIQDUaDTwg6nc79BOvlNKU+x1ISsxDiSkh7WgghBCAJQQghRAlJCEIIIYAG+ByCEEKI66PJtBBmzpxZ11WodRJz0yAxN361FW+TSQhCCCEuTxKCEEIIAAxz5syZU9eVqC2lC640JRJz0yAxN361Ea8MKgshhACky0gIIUQJSQhCCCGARjCX0aUyMjJYsWIFmqYxZMgQjzWOAex2O2+++SaHDh0iICCARx55hPDw8Dqqbc2oKuZPP/2UTZs2YTAYCAwM5MEHH6RZs2Z1VNuaUVXMpbZt28Zrr73Giy++SHR0dC3XsuZUJ96tW7fywQcfoNPpiIqKYvr06XVQ05pTVcxnzpxhwYIF5Ofno2ka48aNo1u3bnVU25rx1ltvsXPnToKCgnj11VfLva6UYsWKFfznP//BbDbz0EMP1ezYgmpEnE6nevjhh9XJkyeV3W5Xjz32mDp69KhHmS+++EItXrxYKaXUt99+q1577bW6qGqNqU7Mu3fvVjabTSml1MaNG5tEzEopVVBQoJ5++mn15JNPqoMHD9ZBTWtGdeI9fvy4+p//+R+Vl5enlFLq3LlzdVHVGlOdmBctWqQ2btyolFLq6NGj6qGHHqqLqtaovXv3qszMTDVjxowKX9+xY4eaO3eu0jRN7d+/XyUlJdXo+zeqLqODBw8SGRlJREQERqORPn36kJaW5lEmPT2dQYMGAdCrVy/27NnToBdRqU7MnTt3xmw2A3DjjTeSk5NTF1WtMdWJGSA5OZnbb78dk8lUB7WsOdWJd9OmTdxyyy34+/sDEBQUVBdVrTHViVmn01FQUAC4VsYLCQmpi6rWqNjYWPfvsCLp6ekMGDAAnU5Hhw4dyM/P5+zZszX2/o0qIeTk5BAaGureDg0NLXfxK1vGYDDg6+tLXl5erdazJlUn5rI2b95M165da6Nq1011Yj58+DBnzpzh5ptvru3q1bjqxHv8+HFOnDjBU089xaxZs8jIyKjtatao6sT829/+lm+++YY///nPvPjii9x33321Xc1al5OTQ1hYmHu7qv/3K9WoEkJFn/QvXTSnOmUakiuJ5+uvv+bQoUPcfvvt17ta11VVMWuaxsqVK/nDH/5Qm9W6bqrzO9Y0jRMnTvDMM88wffp0Fi1aRH5+fm1VscZVJ+bvvvuOQYMGsWjRIpKSknjjjTfQNK22qlgnrvf1q1ElhNDQULKzs93b2dnZ5ZqRZcs4nU4KCgou20Sr76oTM8CuXbtYt24djz/+eIPvQqkqZpvNxtGjR/nf//1fpkyZwoEDB3jppZfIzMysi+pes+r8ji0WC927d8doNBIeHk6LFi04ceJEbVe1xlQn5s2bN9O7d28AOnTogN1ub9Ct/eoIDQ31WACqsv/3q9WoEkJ0dDQnTpwgKysLh8PB1q1bsVqtHmVuvvlmtmzZArjuQImLi2vQLYTqxHz48GGWLl3K448/3uD7lqHqmH19fVm2bBkLFixgwYIF3HjjjTz++OMN9i6j6vyOe/TowZ49ewDIzc3lxIkTRERE1EV1a0R1Yg4LC3PHfOzYMex2O4GBgXVR3VpjtVr5+uuvUUrx888/4+vrW6MJodE9qbxz505WrlyJpmkkJiYyZswYkpOTiY6Oxmq1UlxczJtvvsnhw4fx9/fnkUceadD/OFB1zM899xy//PILwcHBgOsf6YknnqjjWl+bqmIua86cOUyYMKHBJgSoOl6lFO+88w4ZGRno9XrGjBlD375967ra16SqmI8dO8bixYux2WwA3HPPPdx00011XOtrM3/+fPbt20deXh5BQUH87ne/w+FwADBs2DCUUixbtowffvgBLy8vHnrooRr9u250CUEIIcTVaVRdRkIIIa6eJAQhhBCAJAQhhBAlJCEIIYQAJCEIIYQoIQlBCCEEIAlBCCFEif8P0u4lFL8pIBoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history.history['binary_accuracy'], 'g.-', label = 'Binary Accuracy')\n",
    "plt.plot(model.history.history['val_binary_accuracy'], 'r.-', label = 'Val Binary Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Binary Accuracy over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-555d933938d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g.-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Precision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_precision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r.-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Val Precision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision over Time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'precision'"
     ]
    }
   ],
   "source": [
    "plt.plot(model_fit.history['precision'], 'g.-', label = 'Precision')\n",
    "plt.plot(model_fit.history['val_precision'], 'r.-', label = 'Val Precision')\n",
    "plt.legend()\n",
    "plt.title('Precision over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## James' Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(256, 256, 3)),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='softmax'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer = SGD(lr = 1e-6), \n",
    "              metrics = [keras.metrics.BinaryAccuracy(), \n",
    "                   keras.metrics.Precision(), \n",
    "                   keras.metrics.Recall()],\n",
    "              loss = keras.losses.BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1000 steps, validate for 200 steps\n",
      " 411/1000 [===========>..................] - ETA: 3:40 - loss: 2.1835 - binary_accuracy: 0.5003 - precision_10: 0.0000e+00 - recall_10: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-8df1f2e74938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model_fit = model.fit(train_imgs,\n\u001b[1;32m      2\u001b[0m                       \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                       validation_data = val_imgs)\n\u001b[0m",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/apps/anaconda3/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_fit = model.fit(train_imgs,\n",
    "                      epochs = 1,\n",
    "                      validation_data = val_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need 2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.keras.preprocessing' has no attribute 'image_dataset_from_directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-99c35a2ed01f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tf.keras.preprocessing.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_core.keras.preprocessing' has no attribute 'image_dataset_from_directory'"
     ]
    }
   ],
   "source": [
    "tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=\"data\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=None,\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path attempts, no need currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save images as numpy array\n",
    "for dsplit in ('train', 'valid', 'test'):\n",
    "    imgs_to_numpy(dsplit=dsplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['path'].tail()\n",
    "img_path = 'train/fake/73ILM40K3Z.jpg'\n",
    "img_idx = 99999\n",
    "img = plt.imread(DATADIR / img_path)\n",
    "X[img_idx, :, :, :] = img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsplit = 'train'\n",
    "\n",
    "# Check that the split exists\n",
    "if dsplit not in ('train', 'valid', 'test'):\n",
    "    raise Exception('dsplit must be `train`, `test`, or `valid`')\n",
    "\n",
    "# Load labeled dataframes\n",
    "PATHDIR = Path('data')\n",
    "DATADIR = Path('data/') / 'real_vs_fake' / 'real-vs-fake'\n",
    "SAVEPATH = PATHDIR / 'data_array'\n",
    "\n",
    "df = pd.read_csv(PATHDIR / f'{dsplit}.csv', header=0).drop(\n",
    "    ['original_path', 'Unnamed: 0', 'label_str'], axis=1)\n",
    "\n",
    "# Create containers for the image data\n",
    "n = df.shape[0]\n",
    "X = np.empty(shape=(n, 256, 256, 3))\n",
    "y = df['label'].to_numpy()[np.newaxis].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[img_idx, :, :, :]\n",
    "with open(SAVEPATH / f'X_{dsplit}.npy', 'wb') as file:\n",
    "    np.save(file, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAVEPATH / f'y_{dsplit}.npy', 'wb') as file:\n",
    "        np.save(file, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(n, 256, 256, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the training data\n",
    "PATHTRAIN = Path('data') / 'data_array'\n",
    "with tf.device('/device:GPU:0'):\n",
    "    with open(PATHTRAIN / 'X_train.npy', 'rb') as f:\n",
    "        X = np.load(f)\n",
    "    \n",
    "    with open(PATHTRAIN / 'y_train.npy', 'rb') as f:\n",
    "        y = np.load(f)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "d2a653ea5a11e463c7a5ca7f1200831bd9d8f6e90aec136b3f9ac2ea2b6fabb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
